{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAR Earth System Data Science WIP Talk\n",
    "__This presentation is based on work I did during the [NCAR Summer Internship in Parallel Computational Science (SIParCS) program](https://www2.cisl.ucar.edu/siparcs)__\n",
    "### Lucas Sterzinger -- Atmospheric Science PhD Candidate at UC Davis\n",
    "* [Twitter](https://twitter.com/lucassterzinger)\n",
    "* [GitHub](https://github.com/lsterzinger)\n",
    "* [Website](https://lucassterzinger.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Motivation:\n",
    "* NetCDF is not cloud optimized\n",
    "* Other formats, like Zarr, aim to make accessing and reading data from the cloud fast and painless\n",
    "* However, most geoscience datasets available in the cloud are still in their native NetCDF/HDF5, so a different access method is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do I mean when I say \"Cloud Optimized\"?\n",
    "![Move to cloud diagram](images/cloud-move.png)\n",
    "\n",
    "In traditional scientific workflows, data is archived in a repository and downloaded to a separate computer for analysis (left). However, datasets are becoming much too large to fit on personal computers, and transferring full datasets from an archive to a seperate machine can use lots of bandwidth.\n",
    "\n",
    "In a cloud environment, the data can live in object storage (e.g. AWS S3), and analysis can be done in an adjacent compute instances, allowing for low-latency and high-bandwith access to the dataset.\n",
    "\n",
    "## Why NetCDF doesn't work well in this workflow\n",
    "\n",
    "NetCDF is probably the most common binary data format for atmospheric/earth sciences, and has a lot of official and community support. However, the NetCDF format/API requires either a) loading the entire dataset in order to access the header/metadata and retreive a chunk of data or b) use a serverside utility like THREDDS/OPeNDAP/\n",
    "\n",
    "![NetCDF File Object](images/single_file_object.png)\n",
    "\n",
    "## The Zarr Solution\n",
    "The [Zarr data format](https://zarr.readthedocs.io/en/stable/) alleviates this problem by storing the metadata and chunks in seperate files that can be accessed as-needed and in parallel.\n",
    "\n",
    "![Zarr](images/zarr.png)\n",
    "\n",
    "## _However_\n",
    "While Zarr proves to be very good for this cloud-centric workflow, most cloud-available data is currently only available in NetCDF/HDF5/GRIB2 format. While it would be _wonderful_ if all this data converted to Zarr overnight, it would be great if in the meantime there was a way to use some of the Zarr spec, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducting `fsspec-reference-maker`\n",
    "[Github page](https://github.com/intake/fsspec-reference-maker)\n",
    "\n",
    "`fsspec-reference-maker` works by analysing the NetCDF header/metadata info, extracting byte-ranges for each variable chunk, and creating a Zarr-spec metadata file. This file is plaintext and can opened and analyzed with xarray very quickly. When a user requests a certain chunk of data, the NetCDF4 API is bypassed entirely and the Zarr API is used to extract the specified byte-range.\n",
    "\n",
    "![reference-maker vs zarr](images/referencemaker_v_zarr.png)\n",
    "\n",
    "## How much of a difference does this make, really?\n",
    "Testing this method on 24 hours of 5-minute GOES-16 data and accessing via native NetCDF, Zarr, and NetCDF + ReferenceMaker:\n",
    "\n",
    "![workflow results](images/workflow_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `fsspec-reference-maker` and make sure it's at the latest version (`0.0.3` at the time of writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec_reference_maker\n",
    "fsspec_reference_maker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from fsspec_reference_maker.hdf import SingleHdf5ToZarr\n",
    "from fsspec_reference_maker.combine import MultiZarrToZarr\n",
    "import fsspec\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fsspec` -- What is it?\n",
    "* Provides unified interface to different filesystem types\n",
    "* Local, cloud, http, dropbox, Google Drive, etc\n",
    "    * All accessible with the same API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file', 'memory', 'dropbox', 'http', 'https', 'zip', 'tar', 'gcs', 'gs', 'gdrive', 'sftp', 'ssh', 'ftp', 'hdfs', 'arrow_hdfs', 'webhdfs', 's3', 's3a', 'wandb', 'oci', 'adl', 'abfs', 'az', 'cached', 'blockcache', 'filecache', 'simplecache', 'dask', 'dbfs', 'github', 'git', 'smb', 'jupyter', 'jlab', 'libarchive', 'reference'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fsspec.registry import known_implementations\n",
    "known_implementations.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open a new filesystem, of type `s3` (Amazon Web Services storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('s3', anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `fs.glob()` to generate a list of files in a certain directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = fs.glob(\"s3://noaa-goes16/ABI-L2-SSTF/2020/210/*/*.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepend `s3://` to the URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ['s3://' + f for f in flist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a dask cluster\n",
    "[Dask](https://dask.org/) is a python package that allows for easily parallelizing python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definte function to return a reference dictionary for a given S3 file URL\n",
    "\n",
    "This function does the following:\n",
    "1. `so` is a dictionary of options for `fsspec.open()`\n",
    "2. Use `fsspec.open()` to open the file given by URL `f`\n",
    "3. Using `fsspec-reference-maker.SingleHdf5ToZarr()` and supplying the file object `infile` and URL `f`, generate reference with `.translate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ref(f):\n",
    "    so = dict(\n",
    "        mode=\"rb\", anon=True, default_fill_cache=False, default_cache_type=\"none\"\n",
    "    )\n",
    "    with fsspec.open(f, **so) as infile:\n",
    "        return SingleHdf5ToZarr(infile, f, inline_threshold=300).translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map `gen_ref` to each member of `flist_bag` and compute\n",
    "_Note: if running interactively on Binder, this will take a while since only one worker is available and the references will have to be generated in serial. See option for loading from jsons below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "flist_bag = db.from_sequence(flist, npartitions=len(flist))\n",
    "flist_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time dicts = flist_bag.map(gen_ref).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### _Save/load references to/from JSON files (optional)_\n",
    "The individual dictionaries can be saved as JSON files if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# for d in dicts:\n",
    "#     # Generate name from corresponding URL:\n",
    "#     # Grab URL, strip everything but the filename, \n",
    "#     # and replace .nc with .json\n",
    "#     name = d['templates']['u'].split('/')[-1].replace('.nc', '.json')\n",
    "\n",
    "#     with open(name, 'w') as outf:\n",
    "#         outf.write(ujson.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These generated jsons can then be loaded back in as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ujson\n",
    "# dicts = []\n",
    "\n",
    "# for f in sorted(glob('./example_jsons/individual/*.json')):\n",
    "#     with open(f,'r') as fin:\n",
    "#         dicts.append(ujson.load(fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `MultiZarrToZarr` to combine the 24 individual references into a single reference\n",
    "In this example we passed a list of reference dictionaries, but you can also give it a list of `.json` filepaths (commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(\n",
    "    dicts,\n",
    "    # sorted((glob('./example_jsons/individual/*.json'))),\n",
    "    remote_protocol='s3',\n",
    "    remote_options={'anon':True},\n",
    "    xarray_open_kwargs={\n",
    "        \"decode_cf\" : False,\n",
    "        \"mask_and_scale\" : False,\n",
    "        \"decode_times\" : False,\n",
    "        \"decode_timedelta\" : False,\n",
    "        \"use_cftime\" : False,\n",
    "        \"decode_coords\" : False\n",
    "    },\n",
    "    xarray_concat_args={'dim' : 't'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References can be saved to a file (`combined.json`) or passed back as a dictionary (`mzz_dict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time mzz.translate('./combined.json')\n",
    "# mzz_dict = mzz.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d853cbf2f35f45a59f79ca5e397d8dd1594080251b0b51418fe33f5fb0138a7a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
